{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecb7a1-274c-4a9f-b8b6-74989cff63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMonitor:\n",
    "    \"\"\"GPU使用率とメモリ使用量をモニタリング\"\"\"\n",
    "    \n",
    "    def __init__(self, interval=0.1):\n",
    "        self.interval = interval\n",
    "        self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        self.monitoring = False\n",
    "        self.timestamps = deque(maxlen=10000)\n",
    "        self.gpu_utils = deque(maxlen=10000)\n",
    "        self.mem_utils = deque(maxlen=10000)\n",
    "        self.mem_used = deque(maxlen=10000)\n",
    "        self.thread = None\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"モニタリングループ\"\"\"\n",
    "        start_time = time.time()\n",
    "        while self.monitoring:\n",
    "            current_time = time.time() - start_time\n",
    "            \n",
    "            # GPU使用率\n",
    "            util = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)\n",
    "            self.gpu_utils.append(util.gpu)\n",
    "            \n",
    "            # メモリ使用量\n",
    "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)\n",
    "            self.mem_utils.append(100 * mem_info.used / mem_info.total)\n",
    "            self.mem_used.append(mem_info.used / (1024**3))  # GB\n",
    "            \n",
    "            self.timestamps.append(current_time)\n",
    "            time.sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"モニタリング開始\"\"\"\n",
    "        self.monitoring = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"モニタリング停止\"\"\"\n",
    "        self.monitoring = False\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"データ取得\"\"\"\n",
    "        return {\n",
    "            'timestamps': list(self.timestamps),\n",
    "            'gpu_utils': list(self.gpu_utils),\n",
    "            'mem_utils': list(self.mem_utils),\n",
    "            'mem_used_gb': list(self.mem_used)\n",
    "        }\n",
    "    \n",
    "    def plot(self, title=\"GPU Monitoring Results\"):\n",
    "        \"\"\"結果のプロット\"\"\"\n",
    "        data = self.get_data()\n",
    "        if not data['timestamps']:\n",
    "            print(\"No monitoring data available\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "        \n",
    "        # GPU使用率\n",
    "        ax1.plot(data['timestamps'], data['gpu_utils'], 'b-', linewidth=1.5)\n",
    "        ax1.set_ylabel('GPU Utilization (%)')\n",
    "        ax1.set_ylim(0, 105)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_title(f\"{title} - GPU Utilization\")\n",
    "        \n",
    "        # メモリ使用量\n",
    "        ax2.plot(data['timestamps'], data['mem_used_gb'], 'r-', linewidth=1.5, label='Used')\n",
    "        ax2.axhline(y=24, color='k', linestyle='--', alpha=0.5, label='Total (24GB)')\n",
    "        ax2.set_xlabel('Time (seconds)')\n",
    "        ax2.set_ylabel('GPU Memory (GB)')\n",
    "        ax2.set_ylim(0, 26)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        ax2.set_title(\"GPU Memory Usage\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 統計情報\n",
    "        print(f\"\\nGPU Utilization Statistics:\")\n",
    "        print(f\"  Average: {np.mean(data['gpu_utils']):.1f}%\")\n",
    "        print(f\"  Maximum: {np.max(data['gpu_utils']):.1f}%\")\n",
    "        print(f\"  Minimum: {np.min(data['gpu_utils']):.1f}%\")\n",
    "        \n",
    "        print(f\"\\nGPU Memory Statistics:\")\n",
    "        print(f\"  Average: {np.mean(data['mem_used_gb']):.2f} GB\")\n",
    "        print(f\"  Maximum: {np.max(data['mem_used_gb']):.2f} GB\")\n",
    "        print(f\"  Minimum: {np.min(data['mem_used_gb']):.2f} GB\")\n",
    "\n",
    "# Helper functions\n",
    "def print_rmm_statistics():\n",
    "    \"\"\"RMM統計情報を表示\"\"\"\n",
    "    stats = rmm.statistics.get_statistics()\n",
    "    print(\"\\nRMM Memory Statistics:\")\n",
    "    print(f\"  Current allocated: {stats.current_bytes / (1024**3):.2f} GB\")\n",
    "    print(f\"  Peak allocated: {stats.peak_bytes / (1024**3):.2f} GB\")\n",
    "    print(f\"  Total allocations: {stats.n_allocations}\")\n",
    "    print(f\"  Total deallocations: {stats.n_deallocations}\")\n",
    "\n",
    "def print_cupy_memory_info():\n",
    "    \"\"\"CuPyメモリプール情報を表示\"\"\"\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    print(\"\\nCuPy Memory Pool:\")\n",
    "    print(f\"  Used: {mempool.used_bytes() / (1024**3):.2f} GB\")\n",
    "    print(f\"  Total: {mempool.total_bytes() / (1024**3):.2f} GB\")\n",
    "\n",
    "# モニターのインスタンス化\n",
    "monitor = GPUMonitor(interval=0.05)\n",
    "print(\"GPU Monitor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32038b08-5dd8-4d8e-aac0-4430b7102512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMonitor:\n",
    "    \"\"\"GPU使用率とメモリ使用量をモニタリング\"\"\"\n",
    "    \n",
    "    def __init__(self, interval=0.1):\n",
    "        self.interval = interval\n",
    "        self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        self.monitoring = False\n",
    "        self.timestamps = deque(maxlen=10000)\n",
    "        self.gpu_utils = deque(maxlen=10000)\n",
    "        self.mem_utils = deque(maxlen=10000)\n",
    "        self.mem_used = deque(maxlen=10000)\n",
    "        self.thread = None\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"モニタリングループ\"\"\"\n",
    "        start_time = time.time()\n",
    "        while self.monitoring:\n",
    "            current_time = time.time() - start_time\n",
    "            \n",
    "            # GPU使用率\n",
    "            util = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)\n",
    "            self.gpu_utils.append(util.gpu)\n",
    "            \n",
    "            # メモリ使用量\n",
    "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.gpu_handle)\n",
    "            self.mem_utils.append(100 * mem_info.used / mem_info.total)\n",
    "            self.mem_used.append(mem_info.used / (1024**3))  # GB\n",
    "            \n",
    "            self.timestamps.append(current_time)\n",
    "            time.sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"モニタリング開始\"\"\"\n",
    "        self.monitoring = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"モニタリング停止\"\"\"\n",
    "        self.monitoring = False\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"データ取得\"\"\"\n",
    "        return {\n",
    "            'timestamps': list(self.timestamps),\n",
    "            'gpu_utils': list(self.gpu_utils),\n",
    "            'mem_utils': list(self.mem_utils),\n",
    "            'mem_used_gb': list(self.mem_used)\n",
    "        }\n",
    "    \n",
    "    def plot(self, title=\"GPU Monitoring Results\"):\n",
    "        \"\"\"結果のプロット\"\"\"\n",
    "        data = self.get_data()\n",
    "        if not data['timestamps']:\n",
    "            print(\"No monitoring data available\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "        \n",
    "        # GPU使用率\n",
    "        ax1.plot(data['timestamps'], data['gpu_utils'], 'b-', linewidth=1.5)\n",
    "        ax1.set_ylabel('GPU Utilization (%)')\n",
    "        ax1.set_ylim(0, 105)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_title(f\"{title} - GPU Utilization\")\n",
    "        \n",
    "        # メモリ使用量\n",
    "        ax2.plot(data['timestamps'], data['mem_used_gb'], 'r-', linewidth=1.5, label='Used')\n",
    "        ax2.axhline(y=24, color='k', linestyle='--', alpha=0.5, label='Total (24GB)')\n",
    "        ax2.set_xlabel('Time (seconds)')\n",
    "        ax2.set_ylabel('GPU Memory (GB)')\n",
    "        ax2.set_ylim(0, 26)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        ax2.set_title(\"GPU Memory Usage\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 統計情報\n",
    "        print(f\"\\nGPU Utilization Statistics:\")\n",
    "        print(f\"  Average: {np.mean(data['gpu_utils']):.1f}%\")\n",
    "        print(f\"  Maximum: {np.max(data['gpu_utils']):.1f}%\")\n",
    "        print(f\"  Minimum: {np.min(data['gpu_utils']):.1f}%\")\n",
    "        \n",
    "        print(f\"\\nGPU Memory Statistics:\")\n",
    "        print(f\"  Average: {np.mean(data['mem_used_gb']):.2f} GB\")\n",
    "        print(f\"  Maximum: {np.max(data['mem_used_gb']):.2f} GB\")\n",
    "        print(f\"  Minimum: {np.min(data['mem_used_gb']):.2f} GB\")\n",
    "\n",
    "# Helper functions\n",
    "def print_rmm_statistics():\n",
    "    \"\"\"RMM統計情報を表示\"\"\"\n",
    "    try:\n",
    "        stats = rmm.statistics.get_statistics()\n",
    "        if stats:\n",
    "            print(\"\\nRMM Memory Statistics:\")\n",
    "            print(f\"  Current allocated: {stats.current_bytes / (1024**3):.2f} GB\")\n",
    "            print(f\"  Peak allocated: {stats.peak_bytes / (1024**3):.2f} GB\")\n",
    "            print(f\"  Total allocations: {stats.n_allocations}\")\n",
    "            print(f\"  Total deallocations: {stats.n_deallocations}\")\n",
    "        else:\n",
    "            print(\"\\nRMM Memory Statistics: No statistics available\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nRMM Memory Statistics: Error - {e}\")\n",
    "\n",
    "def print_cupy_memory_info():\n",
    "    \"\"\"CuPyメモリプール情報を表示\"\"\"\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    print(\"\\nCuPy Memory Pool:\")\n",
    "    print(f\"  Used: {mempool.used_bytes() / (1024**3):.2f} GB\")\n",
    "    print(f\"  Total: {mempool.total_bytes() / (1024**3):.2f} GB\")\n",
    "\n",
    "# モニターのインスタンス化\n",
    "monitor = GPUMonitor(interval=0.05)\n",
    "print(\"GPU Monitor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2ef55-5986-45db-ab44-b87530940188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU monitoring tools\n",
    "try:\n",
    "    import pynvml\n",
    "except ImportError:\n",
    "    !pip install nvidia-ml-py3\n",
    "    import pynvml\n",
    "\n",
    "import cupy as cp\n",
    "import rmm\n",
    "from rmm.statistics import ProfilerRecords, statistics\n",
    "import threading\n",
    "from collections import deque\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Initialize NVML for GPU monitoring\n",
    "pynvml.nvmlInit()\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Get GPU info\n",
    "gpu_name = pynvml.nvmlDeviceGetName(gpu_handle).decode('utf-8')\n",
    "gpu_mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"GPU Memory: {gpu_mem_info.total / (1024**3):.1f} GB\")\n",
    "\n",
    "# RMM statistics を有効化\n",
    "rmm.statistics.enable_statistics()\n",
    "\n",
    "# RMM を再初期化（統計機能付き）\n",
    "rmm.reinitialize(\n",
    "    managed_memory=False,  # まずは通常のGPUメモリで試す\n",
    "    pool_allocator=True,\n",
    "    initial_pool_size=2<<30,    # 2GB\n",
    "    maximum_pool_size=22<<30,   # 22GB (24GBの GPU用)\n",
    "    logging=True\n",
    ")\n",
    "\n",
    "print(\"\\nRMM memory profiling enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66473811-29f7-4e10-ae64-bc19d33c881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU monitoring tools\n",
    "try:\n",
    "    import pynvml\n",
    "except ImportError:\n",
    "    !pip install nvidia-ml-py3\n",
    "    import pynvml\n",
    "\n",
    "import cupy as cp\n",
    "import rmm\n",
    "from rmm.statistics import ProfilerRecords, statistics\n",
    "import threading\n",
    "from collections import deque\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Initialize NVML for GPU monitoring\n",
    "pynvml.nvmlInit()\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Get GPU info\n",
    "gpu_name_raw = pynvml.nvmlDeviceGetName(gpu_handle)\n",
    "gpu_name = gpu_name_raw.decode('utf-8') if isinstance(gpu_name_raw, bytes) else gpu_name_raw\n",
    "gpu_mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"GPU Memory: {gpu_mem_info.total / (1024**3):.1f} GB\")\n",
    "\n",
    "# RMM statistics を有効化\n",
    "rmm.statistics.enable_statistics()\n",
    "\n",
    "# RMM を再初期化（統計機能付き）\n",
    "rmm.reinitialize(\n",
    "    managed_memory=False,  # まずは通常のGPUメモリで試す\n",
    "    pool_allocator=True,\n",
    "    initial_pool_size=2<<30,    # 2GB\n",
    "    maximum_pool_size=22<<30,   # 22GB (24GBの GPU用)\n",
    "    logging=True\n",
    ")\n",
    "\n",
    "print(\"\\nRMM memory profiling enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f68acb-c385-4896-9b18-eb30cc4feba8",
   "metadata": {},
   "source": [
    "## 1. 環境設定とインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87556445-bfaa-4df1-aac6-0fd32d69ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import decoders\n",
    "from gpu_can_decoder import GPUCANDecoder\n",
    "from cpu_can_decoder import CPUCANDecoder\n",
    "\n",
    "print(\"ライブラリのインポート完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6831e-0570-496c-97cc-0a3d707e258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_can_data(n_messages):\n",
    "    \"\"\"合成CANデータの生成（OpenPilot DBCファイルに準拠）\"\"\"\n",
    "    # リアルなCANデータ分布を模倣\n",
    "    address_distribution = {\n",
    "        170: 0.037,  # 4輪速度\n",
    "        37: 0.037,   # ステアリング\n",
    "        36: 0.037,\n",
    "        740: 0.044,\n",
    "        608: 0.022,\n",
    "        180: 0.018,\n",
    "    }\n",
    "    \n",
    "    # アドレスを生成\n",
    "    addresses = []\n",
    "    for addr, prob in address_distribution.items():\n",
    "        count = int(n_messages * prob)\n",
    "        addresses.extend([addr] * count)\n",
    "    \n",
    "    # 残りはランダムなアドレス\n",
    "    remaining = n_messages - len(addresses)\n",
    "    other_addresses = np.random.choice([452, 466, 467, 705, 321, 562], remaining)\n",
    "    addresses.extend(other_addresses)\n",
    "    \n",
    "    # シャッフル\n",
    "    np.random.shuffle(addresses)\n",
    "    addresses = np.array(addresses[:n_messages], dtype=np.int64)\n",
    "    \n",
    "    # タイムスタンプ（実データと同じ範囲）\n",
    "    timestamps = np.linspace(46408.0, 46468.0, n_messages)\n",
    "    \n",
    "    # データバイト\n",
    "    data_bytes = np.zeros((n_messages, 8), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(n_messages):\n",
    "        if addresses[i] == 170:  # 4輪速度\n",
    "            # OpenPilot DBC: (0.01,-67.67) \"kph\" for Toyota RAV4\n",
    "            for j in range(4):\n",
    "                speed_kmh = np.random.uniform(55, 65)  # 55-65 km/h\n",
    "                raw_value = int((speed_kmh + 67.67) / 0.01)\n",
    "                data_bytes[i, j*2] = (raw_value >> 8) & 0xFF\n",
    "                data_bytes[i, j*2 + 1] = raw_value & 0xFF\n",
    "        elif addresses[i] == 37:  # ステアリング\n",
    "            # 固定値パターン（実データと同じ）\n",
    "            data_bytes[i] = [0x00, 0x00, 0x10, 0x00, 0xC0, 0x00, 0x00, 0xFD]\n",
    "        else:\n",
    "            # その他はランダム\n",
    "            data_bytes[i] = np.random.randint(0, 256, 8, dtype=np.uint8)\n",
    "    \n",
    "    return timestamps, addresses, data_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84181e-a021-4018-bbaa-cbd164223a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生CANデータの構造を確認\n",
    "print(\"=== 生CANデータの列指向配列レイアウト ===\\n\")\n",
    "\n",
    "# サンプルデータを生成して構造を表示\n",
    "n_sample = 5\n",
    "sample_timestamps, sample_addresses, sample_data_bytes = generate_synthetic_can_data(n_sample)\n",
    "\n",
    "print(\"生CANデータは3つの列指向配列で構成されています：\\n\")\n",
    "\n",
    "print(\"1. タイムスタンプ配列 (timestamps):\")\n",
    "print(f\"   - 型: {sample_timestamps.dtype}\")\n",
    "print(f\"   - 形状: {sample_timestamps.shape}\")\n",
    "print(f\"   - 例: {sample_timestamps}\")\n",
    "\n",
    "print(\"\\n2. アドレス配列 (addresses):\")\n",
    "print(f\"   - 型: {sample_addresses.dtype}\")\n",
    "print(f\"   - 形状: {sample_addresses.shape}\")\n",
    "print(f\"   - 例: {sample_addresses}\")\n",
    "\n",
    "print(\"\\n3. データバイト配列 (data_bytes):\")\n",
    "print(f\"   - 型: {sample_data_bytes.dtype}\")\n",
    "print(f\"   - 形状: {sample_data_bytes.shape}\")\n",
    "print(f\"   - 例:\")\n",
    "for i in range(n_sample):\n",
    "    print(f\"     メッセージ{i}: {sample_data_bytes[i]}\")\n",
    "\n",
    "print(\"\\n=== データレイアウトの特徴 ===\")\n",
    "print(\"- 列指向: 各属性（timestamp, address, data）が個別の配列として格納\")\n",
    "print(\"- 効率的: GPUでの並列処理に適した連続メモリレイアウト\")\n",
    "print(\"- Apache Arrow互換: 列指向フォーマットへの変換が容易\")\n",
    "\n",
    "# メモリレイアウトの可視化\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# 各配列のメモリ表現\n",
    "n_messages = 10\n",
    "bar_height = 0.8\n",
    "\n",
    "# タイムスタンプ配列\n",
    "for i in range(n_messages):\n",
    "    rect = mpatches.Rectangle((i, 0), 0.9, bar_height, \n",
    "                            facecolor='lightblue', edgecolor='black', linewidth=0.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i + 0.45, 0.4, f't{i}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "# アドレス配列\n",
    "for i in range(n_messages):\n",
    "    rect = mpatches.Rectangle((i, 1.2), 0.9, bar_height, \n",
    "                            facecolor='lightgreen', edgecolor='black', linewidth=0.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i + 0.45, 1.6, f'a{i}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "# データバイト配列（2次元）\n",
    "for i in range(n_messages):\n",
    "    for j in range(8):\n",
    "        rect = mpatches.Rectangle((i, 2.4 + j * 0.1), 0.9, 0.08, \n",
    "                                facecolor='lightcoral', edgecolor='black', linewidth=0.5)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "ax.text(-1.5, 0.4, 'timestamps[]', ha='right', va='center', fontweight='bold')\n",
    "ax.text(-1.5, 1.6, 'addresses[]', ha='right', va='center', fontweight='bold')\n",
    "ax.text(-1.5, 2.8, 'data_bytes[][]', ha='right', va='center', fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-2, n_messages)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('生CANデータの列指向配列レイアウト', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 矢印で各メッセージの関係を示す\n",
    "for i in [3, 7]:  # 例として2つのメッセージ\n",
    "    ax.annotate('', xy=(i + 0.45, 2.3), xytext=(i + 0.45, 0.9),\n",
    "                arrowprops=dict(arrowstyle='<->', color='gray', lw=0.5))\n",
    "    ax.text(i + 0.7, 1.5, f'メッセージ{i}', ha='left', va='center', fontsize=7, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 実際のデータサイズ計算\n",
    "print(\"\\n=== メモリ使用量の例 ===\")\n",
    "for n in [10_000, 100_000, 1_000_000, 10_000_000]:\n",
    "    timestamp_size = n * 8  # float64\n",
    "    address_size = n * 8    # int64\n",
    "    data_size = n * 8 * 1   # uint8 * 8\n",
    "    total_size = timestamp_size + address_size + data_size\n",
    "    print(f\"{n:>10,} メッセージ: {total_size / (1024**2):>8.1f} MB\")\n",
    "    print(f\"            - timestamps: {timestamp_size / (1024**2):>6.1f} MB\")\n",
    "    print(f\"            - addresses:  {address_size / (1024**2):>6.1f} MB\")\n",
    "    print(f\"            - data_bytes: {data_size / (1024**2):>6.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfa4a7-a634-4337-90e9-e97e012512bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用データの生成 - 10,000,000メッセージまで拡張\n",
    "test_sizes = [\n",
    "    10_000,        # 10K\n",
    "    50_000,        # 50K\n",
    "    100_000,       # 100K\n",
    "    500_000,       # 500K\n",
    "    1_000_000,     # 1M\n",
    "    5_000_000,     # 5M\n",
    "    10_000_000,    # 10M\n",
    "]\n",
    "print(\"テストデータサイズ:\", test_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f7fe9-7978-4279-a580-fb921ff8ab29",
   "metadata": {},
   "source": [
    "## 1.1 生CANデータの列指向配列レイアウト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905319e-bba8-4bfc-be43-44a9cffdfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Initial Memory State ===\")\n",
    "print_rmm_statistics()\n",
    "print_cupy_memory_info()\n",
    "\n",
    "# GPU info from NVML\n",
    "mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "print(f\"\\nGPU Memory (NVML):\")\n",
    "print(f\"  Total: {mem_info.total / (1024**3):.1f} GB\")\n",
    "print(f\"  Used: {mem_info.used / (1024**3):.2f} GB\")\n",
    "print(f\"  Free: {mem_info.free / (1024**3):.2f} GB\")\n",
    "\n",
    "# RAM info\n",
    "print(f\"\\nSystem RAM:\")\n",
    "print(f\"  Available: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"  Total: {psutil.virtual_memory().total / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e134643-1cc2-479d-a5c4-9ddcbf2ba49d",
   "metadata": {},
   "source": [
    "## 2.1 初期メモリ状態の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c01a7-2cf3-4e6b-8b66-7729bcb14e80",
   "metadata": {},
   "source": [
    "## 1.1 GPU プロファイリング設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdca3b9-a1a7-4b1d-9423-aae1e9e5e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# デコーダーの初期化\n",
    "gpu_decoder = GPUCANDecoder(batch_size=500_000)\n",
    "cpu_decoder = CPUCANDecoder(batch_size=100_000)\n",
    "\n",
    "# ベンチマーク結果格納\n",
    "benchmark_results = []\n",
    "\n",
    "for n_messages in test_sizes:\n",
    "    print(f\"\\n--- {n_messages:,} メッセージの処理 ---\")\n",
    "    \n",
    "    # メモリクリア\n",
    "    gc.collect()\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    \n",
    "    # データ生成\n",
    "    timestamps, addresses, data_bytes = generate_synthetic_can_data(n_messages)\n",
    "    data_size_mb = (timestamps.nbytes + addresses.nbytes + data_bytes.nbytes) / (1024**2)\n",
    "    print(f\"データサイズ: {data_size_mb:.1f} MB\")\n",
    "    \n",
    "    # 初期メモリ状態\n",
    "    print(\"Pre-processing memory:\")\n",
    "    print_rmm_statistics()\n",
    "    print_cupy_memory_info()\n",
    "    \n",
    "    # GPU処理（モニタリング付き）\n",
    "    monitor.start()\n",
    "    \n",
    "    gpu_start = time.time()\n",
    "    with statistics.profiler(name=f\"GPU_decode_{n_messages}\"):\n",
    "        gpu_results = gpu_decoder.decode_batch(timestamps, addresses, data_bytes)\n",
    "        cp.cuda.Stream.null.synchronize()  # GPU同期\n",
    "    gpu_time = time.time() - gpu_start\n",
    "    \n",
    "    monitor.stop()\n",
    "    \n",
    "    # CPU処理（大きいデータは時間がかかるため制限）\n",
    "    if n_messages <= 100_000:\n",
    "        cpu_start = time.time()\n",
    "        cpu_results = cpu_decoder.decode_batch(timestamps, addresses, data_bytes)\n",
    "        cpu_time = time.time() - cpu_start\n",
    "    else:\n",
    "        # 線形推定\n",
    "        cpu_time = benchmark_results[-1]['cpu_time'] * (n_messages / benchmark_results[-1]['n_messages'])\n",
    "    \n",
    "    # モニタリング結果の取得\n",
    "    monitor_data = monitor.get_data()\n",
    "    \n",
    "    # 結果記録\n",
    "    result = {\n",
    "        'n_messages': n_messages,\n",
    "        'data_size_mb': data_size_mb,\n",
    "        'gpu_time': gpu_time,\n",
    "        'cpu_time': cpu_time,\n",
    "        'speedup': cpu_time / gpu_time,\n",
    "        'gpu_throughput': n_messages / gpu_time / 1e6,\n",
    "        'cpu_throughput': n_messages / cpu_time / 1e6,\n",
    "        'gpu_util_avg': np.mean(monitor_data['gpu_utils']) if monitor_data['gpu_utils'] else 0,\n",
    "        'gpu_util_max': np.max(monitor_data['gpu_utils']) if monitor_data['gpu_utils'] else 0,\n",
    "        'mem_used_avg_gb': np.mean(monitor_data['mem_used_gb']) if monitor_data['mem_used_gb'] else 0,\n",
    "        'mem_used_max_gb': np.max(monitor_data['mem_used_gb']) if monitor_data['mem_used_gb'] else 0\n",
    "    }\n",
    "    benchmark_results.append(result)\n",
    "    \n",
    "    print(f\"GPU処理時間: {gpu_time:.4f}秒 ({result['gpu_throughput']:.1f} Mmsg/s)\")\n",
    "    print(f\"CPU処理時間: {cpu_time:.4f}秒 ({result['cpu_throughput']:.1f} Mmsg/s)\")\n",
    "    print(f\"高速化率: {result['speedup']:.1f}x\")\n",
    "    print(f\"GPU使用率: 平均{result['gpu_util_avg']:.1f}%, 最大{result['gpu_util_max']:.1f}%\")\n",
    "    print(f\"GPUメモリ: 平均{result['mem_used_avg_gb']:.2f}GB, 最大{result['mem_used_max_gb']:.2f}GB\")\n",
    "    \n",
    "    # GPU使用率のプロット\n",
    "    monitor.plot(title=f\"GPU Processing - {n_messages:,} messages\")\n",
    "    \n",
    "    # Post-processing memory state\n",
    "    print(\"Post-processing memory:\")\n",
    "    print_rmm_statistics()\n",
    "    print_cupy_memory_info()\n",
    "\n",
    "# DataFrameに変換\n",
    "benchmark_df = pd.DataFrame(benchmark_results)\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f665b-8984-494b-9352-4004bd38f1df",
   "metadata": {},
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Processing time comparison\n",
    "ax1.plot(benchmark_df['n_messages'], benchmark_df['gpu_time'], 'b-o', label='GPU', linewidth=2, markersize=8)\n",
    "ax1.plot(benchmark_df['n_messages'], benchmark_df['cpu_time'], 'r-o', label='CPU', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Messages')\n",
    "ax1.set_ylabel('Processing Time (seconds)')\n",
    "ax1.set_title('Processing Time Comparison')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup ratio\n",
    "ax2.plot(benchmark_df['n_messages'], benchmark_df['speedup'], 'g-o', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Messages')\n",
    "ax2.set_ylabel('Speedup (times)')\n",
    "ax2.set_title('GPU Speedup Ratio')\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# GPU使用率 vs データサイズ\n",
    "ax3.plot(benchmark_df['n_messages'], benchmark_df['gpu_util_avg'], 'b-o', \n",
    "         label='Average', linewidth=2, markersize=8)\n",
    "ax3.plot(benchmark_df['n_messages'], benchmark_df['gpu_util_max'], 'r--o', \n",
    "         label='Maximum', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Number of Messages')\n",
    "ax3.set_ylabel('GPU Utilization (%)')\n",
    "ax3.set_title('GPU Utilization vs Data Size')\n",
    "ax3.set_xscale('log')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# メモリ使用量 vs データサイズ\n",
    "ax4.plot(benchmark_df['n_messages'], benchmark_df['mem_used_avg_gb'], 'g-o', \n",
    "         label='Average', linewidth=2, markersize=8)\n",
    "ax4.plot(benchmark_df['n_messages'], benchmark_df['mem_used_max_gb'], 'm--o', \n",
    "         label='Maximum', linewidth=2, markersize=8)\n",
    "ax4.set_xlabel('Number of Messages')\n",
    "ax4.set_ylabel('GPU Memory (GB)')\n",
    "ax4.set_title('GPU Memory Usage vs Data Size')\n",
    "ax4.set_xscale('log')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Results summary\n",
    "print(\"\\n=== Benchmark Results Summary ===\")\n",
    "print(f\"Maximum speedup: {benchmark_df['speedup'].max():.1f}x\")\n",
    "print(f\"Maximum GPU throughput: {benchmark_df['gpu_throughput'].max():.1f} Mmessages/sec\")\n",
    "print(f\"Average CPU throughput: {benchmark_df['cpu_throughput'].mean():.2f} Mmessages/sec\")\n",
    "print(f\"Average GPU utilization: {benchmark_df['gpu_util_avg'].mean():.1f}%\")\n",
    "print(f\"Peak GPU memory usage: {benchmark_df['mem_used_max_gb'].max():.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
