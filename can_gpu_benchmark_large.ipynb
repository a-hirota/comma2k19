{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大規模CANデータGPU処理ベンチマーク\n",
    "\n",
    "100万、1億、100億メッセージの大規模データでGPU処理性能を評価します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設定とインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete\n",
      "Available RAM: 50.2 GB\n",
      "Total RAM: 62.8 GB\n",
      "\n",
      "GPU Memory Pool:\n",
      "  Used: 0.00 GB\n",
      "  Total: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from gpu_can_decoder import GPUCANDecoder\n",
    "from cpu_can_decoder import CPUCANDecoder\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# System info\n",
    "print(\"Environment setup complete\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "# GPU info\n",
    "import cupy as cp\n",
    "mempool = cp.get_default_memory_pool()\n",
    "print(f\"\\nGPU Memory Pool:\")\n",
    "print(f\"  Used: {mempool.used_bytes() / (1024**3):.2f} GB\")\n",
    "print(f\"  Total: {mempool.total_bytes() / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 大規模データ生成関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data generation...\n",
      "  Generated chunk 1/1\n",
      "Generated 100,000 messages\n",
      "Memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_can_data_chunked(n_messages, chunk_size=10_000_000):\n",
    "    \"\"\"大規模CANデータの生成（チャンク処理版）\"\"\"\n",
    "    # リアルなCANデータ分布\n",
    "    address_distribution = {\n",
    "        170: 0.037,  # 4輪速度\n",
    "        37: 0.037,   # ステアリング\n",
    "        36: 0.037,\n",
    "        740: 0.044,\n",
    "        608: 0.022,\n",
    "        180: 0.018,\n",
    "    }\n",
    "    \n",
    "    # 全体の配列を事前割り当て\n",
    "    timestamps = np.empty(n_messages, dtype=np.float64)\n",
    "    addresses = np.empty(n_messages, dtype=np.int64)\n",
    "    data_bytes = np.empty((n_messages, 8), dtype=np.uint8)\n",
    "    \n",
    "    # チャンクごとに処理\n",
    "    n_chunks = (n_messages + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, n_messages)\n",
    "        chunk_messages = end_idx - start_idx\n",
    "        \n",
    "        # アドレスを生成\n",
    "        chunk_addresses = []\n",
    "        for addr, prob in address_distribution.items():\n",
    "            count = int(chunk_messages * prob)\n",
    "            chunk_addresses.extend([addr] * count)\n",
    "        \n",
    "        # 残りはランダムなアドレス\n",
    "        remaining = chunk_messages - len(chunk_addresses)\n",
    "        other_addresses = np.random.choice([452, 466, 467, 705, 321, 562], remaining)\n",
    "        chunk_addresses.extend(other_addresses)\n",
    "        \n",
    "        # シャッフル\n",
    "        np.random.shuffle(chunk_addresses)\n",
    "        addresses[start_idx:end_idx] = chunk_addresses[:chunk_messages]\n",
    "        \n",
    "        # タイムスタンプ\n",
    "        timestamps[start_idx:end_idx] = np.linspace(\n",
    "            46408.0 + (chunk_idx * 60),\n",
    "            46408.0 + ((chunk_idx + 1) * 60),\n",
    "            chunk_messages\n",
    "        )\n",
    "        \n",
    "        # データバイト\n",
    "        chunk_data = np.zeros((chunk_messages, 8), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(chunk_messages):\n",
    "            if addresses[start_idx + i] == 170:  # 4輪速度\n",
    "                # OpenPilot DBC: (0.01,-67.67) \"kph\" for Toyota RAV4\n",
    "                for j in range(4):\n",
    "                    speed_kmh = np.random.uniform(55, 65)  # 55-65 km/h\n",
    "                    raw_value = int((speed_kmh + 67.67) / 0.01)\n",
    "                    chunk_data[i, j*2] = (raw_value >> 8) & 0xFF\n",
    "                    chunk_data[i, j*2 + 1] = raw_value & 0xFF\n",
    "            elif addresses[start_idx + i] == 37:  # ステアリング\n",
    "                chunk_data[i] = [0x00, 0x00, 0x10, 0x00, 0xC0, 0x00, 0x00, 0xFD]\n",
    "            else:\n",
    "                chunk_data[i] = np.random.randint(0, 256, 8, dtype=np.uint8)\n",
    "        \n",
    "        data_bytes[start_idx:end_idx] = chunk_data\n",
    "        \n",
    "        if chunk_idx % 10 == 0:\n",
    "            print(f\"  Generated chunk {chunk_idx + 1}/{n_chunks}\")\n",
    "            gc.collect()\n",
    "    \n",
    "    return timestamps, addresses, data_bytes\n",
    "\n",
    "# テスト\n",
    "print(\"Testing data generation...\")\n",
    "test_t, test_a, test_d = generate_synthetic_can_data_chunked(100_000)\n",
    "print(f\"Generated {len(test_t):,} messages\")\n",
    "print(f\"Memory usage: {(test_t.nbytes + test_a.nbytes + test_d.nbytes) / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 大規模ベンチマーク実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing with 1,000,000 messages\n",
      "============================================================\n",
      "Required memory: ~0.0 GB\n",
      "Available memory: 50.1 GB\n",
      "\n",
      "Generating synthetic data...\n",
      "  Generated chunk 1/1\n",
      "Data generation time: 6.33 seconds\n",
      "Data size: 22.9 MB (0.02 GB)\n",
      "\n",
      "Running GPU processing...\n",
      "\n",
      "=== Results ===\n",
      "GPU processing time: 0.324 seconds\n",
      "Throughput: 3.1 Mmessages/sec\n",
      "Throughput: 0.07 GB/sec\n",
      "Estimated speedup vs CPU: 1.4x\n",
      "Decoded messages: 111,000\n",
      "\n",
      "============================================================\n",
      "Testing with 100,000,000 messages\n",
      "============================================================\n",
      "Required memory: ~2.2 GB\n",
      "Available memory: 50.0 GB\n",
      "\n",
      "Generating synthetic data...\n",
      "  Generated chunk 1/10\n"
     ]
    }
   ],
   "source": [
    "# デコーダーの初期化（大きめのバッチサイズ）\n",
    "gpu_decoder = GPUCANDecoder(batch_size=10_000_000)\n",
    "\n",
    "# テストサイズ\n",
    "test_sizes = [1_000_000, 100_000_000, 10_000_000_000]\n",
    "benchmark_results = []\n",
    "\n",
    "for n_messages in test_sizes:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with {n_messages:,} messages\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # メモリチェック\n",
    "    required_memory_gb = (n_messages * (8 + 8 + 8*1)) / (1024**3)  # timestamps + addresses + data_bytes\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    \n",
    "    print(f\"Required memory: ~{required_memory_gb:.1f} GB\")\n",
    "    print(f\"Available memory: {available_memory_gb:.1f} GB\")\n",
    "    \n",
    "    if required_memory_gb > available_memory_gb * 0.8:  # 80%以上使用する場合は警告\n",
    "        print(\"WARNING: May not have enough memory. Proceeding with caution...\")\n",
    "    \n",
    "    try:\n",
    "        # データ生成\n",
    "        print(\"\\nGenerating synthetic data...\")\n",
    "        gen_start = time.time()\n",
    "        timestamps, addresses, data_bytes = generate_synthetic_can_data_chunked(n_messages)\n",
    "        gen_time = time.time() - gen_start\n",
    "        \n",
    "        data_size_mb = (timestamps.nbytes + addresses.nbytes + data_bytes.nbytes) / (1024**2)\n",
    "        print(f\"Data generation time: {gen_time:.2f} seconds\")\n",
    "        print(f\"Data size: {data_size_mb:.1f} MB ({data_size_mb/1024:.2f} GB)\")\n",
    "        \n",
    "        # GPU処理\n",
    "        print(\"\\nRunning GPU processing...\")\n",
    "        gpu_start = time.time()\n",
    "        \n",
    "        # メモリプールをクリア\n",
    "        mempool = cp.get_default_memory_pool()\n",
    "        mempool.free_all_blocks()\n",
    "        \n",
    "        gpu_results = gpu_decoder.decode_batch(timestamps, addresses, data_bytes)\n",
    "        cp.cuda.Stream.null.synchronize()  # GPU同期\n",
    "        gpu_time = time.time() - gpu_start\n",
    "        \n",
    "        # 結果の統計\n",
    "        n_decoded = sum(len(df) for df in gpu_results.values() if df is not None)\n",
    "        \n",
    "        # CPU処理時間の推定（線形スケーリング）\n",
    "        # 100万メッセージで約0.45秒という前回の結果から推定\n",
    "        estimated_cpu_time = 0.45 * (n_messages / 1_000_000)\n",
    "        \n",
    "        # 結果記録\n",
    "        result = {\n",
    "            'n_messages': n_messages,\n",
    "            'data_size_gb': data_size_mb / 1024,\n",
    "            'generation_time': gen_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'estimated_cpu_time': estimated_cpu_time,\n",
    "            'speedup': estimated_cpu_time / gpu_time,\n",
    "            'gpu_throughput_mmsg': n_messages / gpu_time / 1e6,\n",
    "            'gpu_throughput_gb': (data_size_mb / 1024) / gpu_time,\n",
    "            'n_decoded_messages': n_decoded\n",
    "        }\n",
    "        benchmark_results.append(result)\n",
    "        \n",
    "        print(f\"\\n=== Results ===\")\n",
    "        print(f\"GPU processing time: {gpu_time:.3f} seconds\")\n",
    "        print(f\"Throughput: {result['gpu_throughput_mmsg']:.1f} Mmessages/sec\")\n",
    "        print(f\"Throughput: {result['gpu_throughput_gb']:.2f} GB/sec\")\n",
    "        print(f\"Estimated speedup vs CPU: {result['speedup']:.1f}x\")\n",
    "        print(f\"Decoded messages: {n_decoded:,}\")\n",
    "        \n",
    "        # メモリクリーンアップ\n",
    "        del timestamps, addresses, data_bytes, gpu_results\n",
    "        gc.collect()\n",
    "        mempool.free_all_blocks()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {str(e)}\")\n",
    "        print(\"Skipping this test size...\")\n",
    "        continue\n",
    "\n",
    "# 結果をDataFrameに\n",
    "if benchmark_results:\n",
    "    benchmark_df = pd.DataFrame(benchmark_results)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BENCHMARK SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(benchmark_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Processing time vs data size\n",
    "    ax1.plot(benchmark_df['n_messages'], benchmark_df['gpu_time'], 'b-o', \n",
    "             label='GPU Actual', linewidth=2, markersize=8)\n",
    "    ax1.plot(benchmark_df['n_messages'], benchmark_df['estimated_cpu_time'], 'r--o', \n",
    "             label='CPU Estimated', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Number of Messages')\n",
    "    ax1.set_ylabel('Processing Time (seconds)')\n",
    "    ax1.set_title('Processing Time Scaling')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Throughput scaling\n",
    "    ax2.plot(benchmark_df['n_messages'], benchmark_df['gpu_throughput_mmsg'], 'g-o', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Number of Messages')\n",
    "    ax2.set_ylabel('Throughput (Mmessages/sec)')\n",
    "    ax2.set_title('GPU Throughput Scaling')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Data throughput (GB/s)\n",
    "    ax3.plot(benchmark_df['n_messages'], benchmark_df['gpu_throughput_gb'], 'm-o', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax3.set_xlabel('Number of Messages')\n",
    "    ax3.set_ylabel('Throughput (GB/sec)')\n",
    "    ax3.set_title('GPU Data Throughput')\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speedup ratio\n",
    "    ax4.plot(benchmark_df['n_messages'], benchmark_df['speedup'], 'c-o', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax4.set_xlabel('Number of Messages')\n",
    "    ax4.set_ylabel('Speedup (times)')\n",
    "    ax4.set_title('Estimated GPU Speedup vs CPU')\n",
    "    ax4.set_xscale('log')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n=== Performance Summary ===\")\n",
    "    print(f\"Maximum throughput: {benchmark_df['gpu_throughput_mmsg'].max():.1f} Mmessages/sec\")\n",
    "    print(f\"Maximum data rate: {benchmark_df['gpu_throughput_gb'].max():.2f} GB/sec\")\n",
    "    print(f\"Maximum speedup: {benchmark_df['speedup'].max():.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. メモリ使用量分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリ使用量の理論値計算\n",
    "print(\"=== Memory Usage Analysis ===\")\n",
    "print(\"\\nPer-message memory requirements:\")\n",
    "print(\"  - Timestamp: 8 bytes (float64)\")\n",
    "print(\"  - Address: 8 bytes (int64)\")\n",
    "print(\"  - Data: 8 bytes (8 x uint8)\")\n",
    "print(\"  - Total: 24 bytes/message\")\n",
    "print(\"\\nDataset sizes:\")\n",
    "\n",
    "for n_messages in test_sizes:\n",
    "    total_gb = (n_messages * 24) / (1024**3)\n",
    "    print(f\"  - {n_messages:,} messages: {total_gb:.2f} GB\")\n",
    "\n",
    "# GPU メモリ情報\n",
    "print(\"\\nGPU Memory Status:\")\n",
    "mempool = cp.get_default_memory_pool()\n",
    "print(f\"  - Used: {mempool.used_bytes() / (1024**3):.2f} GB\")\n",
    "print(f\"  - Total allocated: {mempool.total_bytes() / (1024**3):.2f} GB\")\n",
    "\n",
    "# GPUデバイス情報\n",
    "device = cp.cuda.Device()\n",
    "print(f\"\\nGPU Device Info:\")\n",
    "print(f\"  - Name: {device.name}\")\n",
    "print(f\"  - Total Memory: {device.mem_info[1] / (1024**3):.1f} GB\")\n",
    "print(f\"  - Free Memory: {device.mem_info[0] / (1024**3):.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
