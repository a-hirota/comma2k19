{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大規模CANデータGPU処理ベンチマーク（RMM最適化版）\n",
    "\n",
    "1000万、1億、10億メッセージの大規模データでGPU処理性能を評価します。\n",
    "RMM managed memoryとメモリプール設定を最適化しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設定とRMM初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rmm\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "from gpu_can_decoder import GPUCANDecoder\n",
    "from cpu_can_decoder import CPUCANDecoder\n",
    "\n",
    "# RMM managed memory の有効化（OOM回避のため）\n",
    "# GPUメモリが不足した場合、自動的にホストメモリを使用\n",
    "rmm.reinitialize(\n",
    "    managed_memory=True,\n",
    "    pool_allocator=True,\n",
    "    initial_pool_size=2<<30,  # 2GB initial pool\n",
    "    maximum_pool_size=20<<30  # 20GB max pool (24GB GPUの場合)\n",
    ")\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# System info\n",
    "print(\"Environment setup complete with RMM managed memory\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "# GPU info\n",
    "device = cp.cuda.Device()\n",
    "mem_info = device.mem_info\n",
    "print(f\"\\nGPU Memory:\")\n",
    "print(f\"  Free: {mem_info[0] / (1024**3):.2f} GB\")\n",
    "print(f\"  Total: {mem_info[1] / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 大規模データ生成関数（メモリ効率化版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_can_data_chunked(n_messages, chunk_size=5_000_000):\n",
    "    \"\"\"大規模CANデータの生成（チャンク処理版）\"\"\"\n",
    "    # リアルなCANデータ分布\n",
    "    address_distribution = {\n",
    "        170: 0.037,  # 4輪速度\n",
    "        37: 0.037,   # ステアリング\n",
    "        36: 0.037,\n",
    "        740: 0.044,\n",
    "        608: 0.022,\n",
    "        180: 0.018,\n",
    "    }\n",
    "    \n",
    "    # メモリ効率のため小さめのチャンクサイズを使用\n",
    "    chunk_size = min(chunk_size, n_messages)\n",
    "    \n",
    "    # 全体の配列を事前割り当て\n",
    "    timestamps = np.empty(n_messages, dtype=np.float64)\n",
    "    addresses = np.empty(n_messages, dtype=np.int64)\n",
    "    data_bytes = np.empty((n_messages, 8), dtype=np.uint8)\n",
    "    \n",
    "    # チャンクごとに処理\n",
    "    n_chunks = (n_messages + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, n_messages)\n",
    "        chunk_messages = end_idx - start_idx\n",
    "        \n",
    "        # アドレスを生成\n",
    "        chunk_addresses = []\n",
    "        for addr, prob in address_distribution.items():\n",
    "            count = int(chunk_messages * prob)\n",
    "            chunk_addresses.extend([addr] * count)\n",
    "        \n",
    "        # 残りはランダムなアドレス\n",
    "        remaining = chunk_messages - len(chunk_addresses)\n",
    "        other_addresses = np.random.choice([452, 466, 467, 705, 321, 562], remaining)\n",
    "        chunk_addresses.extend(other_addresses)\n",
    "        \n",
    "        # シャッフル\n",
    "        np.random.shuffle(chunk_addresses)\n",
    "        addresses[start_idx:end_idx] = chunk_addresses[:chunk_messages]\n",
    "        \n",
    "        # タイムスタンプ\n",
    "        timestamps[start_idx:end_idx] = np.linspace(\n",
    "            46408.0 + (chunk_idx * 60),\n",
    "            46408.0 + ((chunk_idx + 1) * 60),\n",
    "            chunk_messages\n",
    "        )\n",
    "        \n",
    "        # データバイト\n",
    "        chunk_data = np.zeros((chunk_messages, 8), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(chunk_messages):\n",
    "            if addresses[start_idx + i] == 170:  # 4輪速度\n",
    "                for j in range(4):\n",
    "                    speed_kmh = np.random.uniform(55, 65)  # 55-65 km/h\n",
    "                    raw_value = int((speed_kmh + 67.67) / 0.01)\n",
    "                    chunk_data[i, j*2] = (raw_value >> 8) & 0xFF\n",
    "                    chunk_data[i, j*2 + 1] = raw_value & 0xFF\n",
    "            elif addresses[start_idx + i] == 37:  # ステアリング\n",
    "                chunk_data[i] = [0x00, 0x00, 0x10, 0x00, 0xC0, 0x00, 0x00, 0xFD]\n",
    "            else:\n",
    "                chunk_data[i] = np.random.randint(0, 256, 8, dtype=np.uint8)\n",
    "        \n",
    "        data_bytes[start_idx:end_idx] = chunk_data\n",
    "        \n",
    "        if chunk_idx % max(1, n_chunks // 10) == 0:\n",
    "            print(f\"  Generated chunk {chunk_idx + 1}/{n_chunks}\")\n",
    "            gc.collect()\n",
    "    \n",
    "    return timestamps, addresses, data_bytes\n",
    "\n",
    "# テスト\n",
    "print(\"Testing data generation...\")\n",
    "test_t, test_a, test_d = generate_synthetic_can_data_chunked(100_000)\n",
    "print(f\"Generated {len(test_t):,} messages\")\n",
    "print(f\"Memory usage: {(test_t.nbytes + test_a.nbytes + test_d.nbytes) / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPUデコーダーの拡張（チャンク処理対応）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkedGPUCANDecoder(GPUCANDecoder):\n",
    "    \"\"\"チャンク処理対応のGPUCANデコーダー\"\"\"\n",
    "    \n",
    "    def decode_large_batch(self, timestamps, addresses, data_bytes, chunk_size=50_000_000):\n",
    "        \"\"\"大規模データのチャンク処理\"\"\"\n",
    "        n_messages = len(timestamps)\n",
    "        n_chunks = (n_messages + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        # 結果を格納する辞書\n",
    "        all_results = {signal: [] for signal in self.signal_configs.keys()}\n",
    "        \n",
    "        for chunk_idx in range(n_chunks):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, n_messages)\n",
    "            \n",
    "            print(f\"  Processing chunk {chunk_idx + 1}/{n_chunks} ({end_idx - start_idx:,} messages)\")\n",
    "            \n",
    "            # チャンクデータの抽出\n",
    "            chunk_timestamps = timestamps[start_idx:end_idx]\n",
    "            chunk_addresses = addresses[start_idx:end_idx]\n",
    "            chunk_data = data_bytes[start_idx:end_idx]\n",
    "            \n",
    "            # チャンクの処理\n",
    "            chunk_results = self.decode_batch(chunk_timestamps, chunk_addresses, chunk_data)\n",
    "            \n",
    "            # 結果の統合\n",
    "            for signal, df in chunk_results.items():\n",
    "                if df is not None and len(df) > 0:\n",
    "                    all_results[signal].append(df)\n",
    "            \n",
    "            # メモリクリーンアップ\n",
    "            del chunk_timestamps, chunk_addresses, chunk_data, chunk_results\n",
    "            gc.collect()\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "        # 全チャンクの結果を結合\n",
    "        final_results = {}\n",
    "        for signal, dfs in all_results.items():\n",
    "            if dfs:\n",
    "                final_results[signal] = cudf.concat(dfs, ignore_index=True)\n",
    "                # タイムスタンプでソート\n",
    "                final_results[signal] = final_results[signal].sort_values('timestamp').reset_index(drop=True)\n",
    "            else:\n",
    "                final_results[signal] = None\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "# デコーダーの初期化\n",
    "gpu_decoder = ChunkedGPUCANDecoder(batch_size=10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 大規模ベンチマーク実行（修正版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストサイズ（変更可能）\n",
    "test_sizes = [10_000_000, 100_000_000, 1_000_000_000]\n",
    "benchmark_results = []\n",
    "\n",
    "for n_messages in test_sizes:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with {n_messages:,} messages\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # メモリチェック\n",
    "    required_memory_gb = (n_messages * 24) / (1024**3)  # 24 bytes per message\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    \n",
    "    print(f\"Required memory: ~{required_memory_gb:.1f} GB\")\n",
    "    print(f\"Available memory: {available_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # 10億メッセージの場合は特別な処理\n",
    "    if n_messages >= 1_000_000_000:\n",
    "        print(\"\\nUsing streaming approach for 1B+ messages...\")\n",
    "        # ストリーミング処理を実装（将来の拡張）\n",
    "        print(\"Skipping 1B test for now (requires streaming implementation)\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # データ生成\n",
    "        print(\"\\nGenerating synthetic data...\")\n",
    "        gen_start = time.time()\n",
    "        timestamps, addresses, data_bytes = generate_synthetic_can_data_chunked(n_messages)\n",
    "        gen_time = time.time() - gen_start\n",
    "        \n",
    "        data_size_mb = (timestamps.nbytes + addresses.nbytes + data_bytes.nbytes) / (1024**2)\n",
    "        print(f\"Data generation time: {gen_time:.2f} seconds\")\n",
    "        print(f\"Data size: {data_size_mb:.1f} MB ({data_size_mb/1024:.2f} GB)\")\n",
    "        \n",
    "        # GPU処理\n",
    "        print(\"\\nRunning GPU processing...\")\n",
    "        gpu_start = time.time()\n",
    "        \n",
    "        # 100M以上の場合はチャンク処理を使用\n",
    "        if n_messages >= 100_000_000:\n",
    "            gpu_results = gpu_decoder.decode_large_batch(timestamps, addresses, data_bytes)\n",
    "        else:\n",
    "            gpu_results = gpu_decoder.decode_batch(timestamps, addresses, data_bytes)\n",
    "        \n",
    "        cp.cuda.Stream.null.synchronize()  # GPU同期\n",
    "        gpu_time = time.time() - gpu_start\n",
    "        \n",
    "        # 結果の統計\n",
    "        n_decoded = sum(len(df) for df in gpu_results.values() if df is not None)\n",
    "        \n",
    "        # CPU処理時間の推定\n",
    "        estimated_cpu_time = 0.45 * (n_messages / 1_000_000)\n",
    "        \n",
    "        # 結果記録\n",
    "        result = {\n",
    "            'n_messages': n_messages,\n",
    "            'data_size_gb': data_size_mb / 1024,\n",
    "            'generation_time': gen_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'estimated_cpu_time': estimated_cpu_time,\n",
    "            'speedup': estimated_cpu_time / gpu_time,\n",
    "            'gpu_throughput_mmsg': n_messages / gpu_time / 1e6,\n",
    "            'gpu_throughput_gb': (data_size_mb / 1024) / gpu_time,\n",
    "            'n_decoded_messages': n_decoded\n",
    "        }\n",
    "        benchmark_results.append(result)\n",
    "        \n",
    "        print(f\"\\n=== Results ===\")\n",
    "        print(f\"GPU processing time: {gpu_time:.3f} seconds\")\n",
    "        print(f\"Throughput: {result['gpu_throughput_mmsg']:.1f} Mmessages/sec\")\n",
    "        print(f\"Throughput: {result['gpu_throughput_gb']:.2f} GB/sec\")\n",
    "        print(f\"Estimated speedup vs CPU: {result['speedup']:.1f}x\")\n",
    "        print(f\"Decoded messages: {n_decoded:,}\")\n",
    "        \n",
    "        # メモリクリーンアップ\n",
    "        del timestamps, addresses, data_bytes, gpu_results\n",
    "        gc.collect()\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"Skipping this test size...\")\n",
    "        continue\n",
    "\n",
    "# 結果をDataFrameに\n",
    "if benchmark_results:\n",
    "    benchmark_df = pd.DataFrame(benchmark_results)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BENCHMARK SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(benchmark_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Processing time vs data size\n",
    "    ax1.plot(benchmark_df['n_messages'], benchmark_df['gpu_time'], 'b-o', \n",
    "             label='GPU Actual', linewidth=2, markersize=8)\n",
    "    ax1.plot(benchmark_df['n_messages'], benchmark_df['estimated_cpu_time'], 'r--o', \n",
    "             label='CPU Estimated', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Number of Messages')\n",
    "    ax1.set_ylabel('Processing Time (seconds)')\n",
    "    ax1.set_title('Processing Time Scaling')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Throughput scaling\n",
    "    ax2.plot(benchmark_df['n_messages'], benchmark_df['gpu_throughput_mmsg'], 'g-o', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Number of Messages')\n",
    "    ax2.set_ylabel('Throughput (Mmessages/sec)')\n",
    "    ax2.set_title('GPU Throughput Scaling')\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Data throughput (GB/s)\n",
    "    ax3.plot(benchmark_df['n_messages'], benchmark_df['gpu_throughput_gb'], 'm-o', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax3.set_xlabel('Number of Messages')\n",
    "    ax3.set_ylabel('Throughput (GB/sec)')\n",
    "    ax3.set_title('GPU Data Throughput')\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speedup ratio\n",
    "    ax4.plot(benchmark_df['n_messages'], benchmark_df['speedup'], 'c-o', \n",
    "             linewidth=2, markersize=8)\n",
    "    ax4.set_xlabel('Number of Messages')\n",
    "    ax4.set_ylabel('Speedup (times)')\n",
    "    ax4.set_title('Estimated GPU Speedup vs CPU')\n",
    "    ax4.set_xscale('log')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n=== Performance Summary ===\")\n",
    "    print(f\"Maximum throughput: {benchmark_df['gpu_throughput_mmsg'].max():.1f} Mmessages/sec\")\n",
    "    print(f\"Maximum data rate: {benchmark_df['gpu_throughput_gb'].max():.2f} GB/sec\")\n",
    "    print(f\"Maximum speedup: {benchmark_df['speedup'].max():.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. メモリ使用量分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリ使用量の理論値計算\n",
    "print(\"=== Memory Usage Analysis ===\")\n",
    "print(\"\\nPer-message memory requirements:\")\n",
    "print(\"  - Timestamp: 8 bytes (float64)\")\n",
    "print(\"  - Address: 8 bytes (int64)\")\n",
    "print(\"  - Data: 8 bytes (8 x uint8)\")\n",
    "print(\"  - Total: 24 bytes/message\")\n",
    "print(\"\\nDataset sizes:\")\n",
    "\n",
    "for n_messages in test_sizes:\n",
    "    total_gb = (n_messages * 24) / (1024**3)\n",
    "    print(f\"  - {n_messages:,} messages: {total_gb:.2f} GB\")\n",
    "\n",
    "# GPU メモリ情報\n",
    "print(\"\\nGPU Memory Status:\")\n",
    "mempool = cp.get_default_memory_pool()\n",
    "print(f\"  - Used: {mempool.used_bytes() / (1024**3):.2f} GB\")\n",
    "print(f\"  - Total allocated: {mempool.total_bytes() / (1024**3):.2f} GB\")\n",
    "\n",
    "# GPUデバイス情報\n",
    "device = cp.cuda.Device()\n",
    "mem_info = device.mem_info\n",
    "print(f\"\\nGPU Device Info:\")\n",
    "print(f\"  - Total Memory: {mem_info[1] / (1024**3):.1f} GB\")\n",
    "print(f\"  - Free Memory: {mem_info[0] / (1024**3):.1f} GB\")\n",
    "print(f\"  - Used Memory: {(mem_info[1] - mem_info[0]) / (1024**3):.1f} GB\")\n",
    "\n",
    "# RMM設定情報\n",
    "print(f\"\\nRMM Configuration:\")\n",
    "print(f\"  - Managed Memory: Enabled\")\n",
    "print(f\"  - Pool Allocator: Enabled\")\n",
    "print(f\"  - Initial Pool Size: 2 GB\")\n",
    "print(f\"  - Maximum Pool Size: 20 GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. パフォーマンス改善の推奨事項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Performance Optimization Recommendations ===\")\n",
    "print(\"\\n1. RMM Managed Memory:\")\n",
    "print(\"   - 現在有効化されており、GPUメモリ不足時に自動的にホストメモリを使用\")\n",
    "print(\"   - パフォーマンスは低下するが、OOMエラーを回避可能\")\n",
    "\n",
    "print(\"\\n2. Chunk Processing:\")\n",
    "print(\"   - 100M以上のメッセージでは自動的にチャンク処理を使用\")\n",
    "print(\"   - チャンクサイズは50Mメッセージ（約1.2GB）に設定\")\n",
    "\n",
    "print(\"\\n3. Memory Pool Settings:\")\n",
    "print(\"   - Initial: 2GB (GPUの高速処理用)\")\n",
    "print(\"   - Maximum: 20GB (24GB GPUの場合の推奨値)\")\n",
    "\n",
    "print(\"\\n4. Further Optimizations:\")\n",
    "print(\"   - ストリーミング処理の実装（10億メッセージ以上）\")\n",
    "print(\"   - マルチGPU対応\")\n",
    "print(\"   - より効率的なメモリ再利用\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}